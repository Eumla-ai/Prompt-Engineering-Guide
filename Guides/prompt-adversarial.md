# Adversarial Prompting

Adversarial prompting is an important topic in prompt engineering as it could help to understand the risks and safety issues involved with LLMs. It's also an important discipline to identify these risks and design techniques to address the issues.

The community has found many different types of adversarial prompts attacks that involve some form of prompt injection. We provide a list of these examples below. 

When you are building LLMs, it's really important to protect against prompt attacks that could bypass safety guardrails and break the guiding principles of the model. We will cover examples of this below.

Please note that it is possible that more robust models have been implemented to address some of the issues documented here. This means that some of the prompt attacks below might not be as effective anymore. 

Topics:
- [Prompt Injection](#prompt-injection)
- [Prompt Injection Workarounds](#prompt-injection-workarounds)
- [Prompt Leaking](#prompt-leaking)
- [Jailbreaking](#jailbreaking)